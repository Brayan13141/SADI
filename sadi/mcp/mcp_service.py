# mcp/mcp_service.py
import torch
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
from django.conf import settings
import logging
import gc
import re

logger = logging.getLogger(__name__)

class OptimizedSpanishMCPService:
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        self.is_loaded = False
        
        # MODELO RECOMENDADO - Cambia esta l√≠nea seg√∫n tu elecci√≥n
        self.model_id = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'  # Mantenemos TinyLlama por ahora
    
    def load_model(self):
        """Carga el modelo optimizado para espa√±ol"""
        if self.is_loaded:
            return
            
        try:
            logger.info(f"üöÄ Cargando {self.model_id}...")
            
            torch.set_grad_enabled(False)
            
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_id,
                cache_dir="./model_cache"
            )
            self.tokenizer.pad_token = self.tokenizer.eos_token
            
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_id,
                torch_dtype=torch.float32,
                device_map=None,
                low_cpu_mem_usage=True,
                trust_remote_code=True,
            )
            
            self.model = self.model.to('cpu')
            self.model.eval()
            
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                max_length=1024,
                temperature=0.3,  # M√°s bajo para menos creatividad
                do_sample=True,
                pad_token_id=self.tokenizer.eos_token_id,
            )
            
            self.is_loaded = True
            logger.info("‚úÖ Modelo cargado correctamente")
            
            gc.collect()
            
        except Exception as e:
            logger.error(f"‚ùå Error cargando modelo: {e}")
            self.setup_fallback()
    
    def setup_fallback(self):
        self.is_loaded = True
    
    def generate_contextual_response(self, prompt, db_context=""):
        """Genera respuesta optimizada para espa√±ol y contexto"""
        if not self.is_loaded:
            self.load_model()
        
        # Si no hay contexto v√°lido, respuesta directa
        if not db_context or any(x in db_context for x in ["Error", "No hay", "Consulta general"]):
            return self.get_spanish_fallback(prompt, db_context)
        
        try:
            # PROMPT MEJORADO - M√°s espec√≠fico y en espa√±ol
            spanish_prompt = self.create_spanish_prompt(prompt, db_context)
            
            # Generaci√≥n con par√°metros optimizados
            with torch.no_grad():
                outputs = self.pipeline(
                    spanish_prompt,
                    max_new_tokens=300,
                    num_return_sequences=1,
                    temperature=0.2,  # Muy bajo para m√°xima precisi√≥n
                    top_p=0.7,
                    repetition_penalty=1.5,  # Alto para evitar repetici√≥n
                    do_sample=False,  # Desactivado para m√°s consistencia
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
            
            response = outputs[0]['generated_text']
            
            # Extraer respuesta
            if spanish_prompt in response:
                response = response.replace(spanish_prompt, '').strip()
            
            # Limpieza mejorada
            response = self.clean_spanish_response(response)
            
            # Verificar si la respuesta es √∫til
            if not self.is_useful_response(response, db_context):
                return self.get_contextual_direct_response(prompt, db_context)
            
            return response
            
        except Exception as e:
            logger.error(f"Error en generaci√≥n: {e}")
            return self.get_contextual_direct_response(prompt, db_context)
    
    def create_spanish_prompt(self, prompt, db_context):
        """Crea prompt optimizado para espa√±ol"""
        prompt_templates = {
            'proyectos': f"""Responde en espa√±ol usando SOLO esta informaci√≥n:

INFORMACI√ìN DE PROYECTOS:
{db_context}

PREGUNTA: {prompt}

RESPUESTA (solo en espa√±ol, solo informaci√≥n de arriba):""",

            'metas': f"""Responde en espa√±ol usando SOLO esta informaci√≥n:

INFORMACI√ìN DE METAS:
{db_context}

PREGUNTA: {prompt}

RESPUESTA (solo en espa√±ol, solo informaci√≥n de arriba):""",

            'actividades': f"""Responde en espa√±ol usando SOLO esta informaci√≥n:

INFORMACI√ìN DE ACTIVIDADES:
{db_context}

PREGUNTA: {prompt}

RESPUESTA (solo en espa√±ol, solo informaci√≥n de arriba):""",

            'ciclos': f"""Responde en espa√±ol usando SOLO esta informaci√≥n:

INFORMACI√ìN DE CICLOS:
{db_context}

PREGUNTA: {prompt}

RESPUESTA (solo en espa√±ol, solo informaci√≥n de arriba):"""
        }
        
        prompt_lower = prompt.lower()
        if 'proyecto' in prompt_lower:
            return prompt_templates['proyectos']
        elif 'meta' in prompt_lower or 'objetivo' in prompt_lower:
            return prompt_templates['metas']
        elif 'actividad' in prompt_lower:
            return prompt_templates['actividades']
        elif 'ciclo' in prompt_lower:
            return prompt_templates['ciclos']
        else:
            return f"Responde en espa√±ol usando SOLO esta informaci√≥n:\n\n{db_context}\n\nPREGUNTA: {prompt}\n\nRESPUESTA:"
    
    def clean_spanish_response(self, response):
        """Limpia respuestas en espa√±ol"""
        # Eliminar frases en ingl√©s
        english_phrases = [
            "please", "find", "information", "here", "specific", "details",
            "according", "data", "following", "list", "here is", "below"
        ]
        
        for phrase in english_phrases:
            response = re.sub(rf'\b{phrase}\b.*?\.', '', response, flags=re.IGNORECASE)
        
        # Eliminar texto repetitivo o inv√°lido
        invalid_patterns = [
            r"Informaci√≥n general.*",
            r"Consulta general.*",
            r"‚ÑπÔ∏è.*INFORMACI√ìN DEL SISTEMA.*",
        ]
        
        for pattern in invalid_patterns:
            response = re.sub(pattern, '', response)
        
        # Si la respuesta qued√≥ muy corta, devolver vac√≠o
        if len(response.strip()) < 10:
            return ""
        
        return response.strip()
    
    def is_useful_response(self, response, db_context):
        """Verifica si la respuesta es √∫til"""
        if not response or len(response) < 15:
            return False
        
        # Verificar que contiene informaci√≥n del contexto
        context_words = set(db_context.lower().split()[:10])
        response_words = set(response.lower().split())
        common_words = context_words.intersection(response_words)
        
        return len(common_words) >= 2
    
    def get_contextual_direct_response(self, prompt, db_context):
        """Respuesta directa en espa√±ol usando el contexto"""
        prompt_lower = prompt.lower()
        
        if 'proyecto' in prompt_lower:
            return f"üèóÔ∏è **TUS PROYECTOS EN SADI:**\n\n{db_context}"
        elif 'meta' in prompt_lower or 'objetivo' in prompt_lower:
            return f"üéØ **TUS METAS EN SADI:**\n\n{db_context}"
        elif 'actividad' in prompt_lower:
            return f"‚úÖ **TUS ACTIVIDADES EN SADI:**\n\n{db_context}"
        elif 'ciclo' in prompt_lower:
            return f"üîÑ **CICLOS EN SADI:**\n\n{db_context}"
        else:
            return f"üìä **INFORMACI√ìN SOLICITADA:**\n\n{db_context}"
    
    def get_spanish_fallback(self, prompt, db_context):
        """Respuesta de respaldo en espa√±ol"""
        prompt_lower = prompt.lower()
        
        if 'proyecto' in prompt_lower:
            return "üìã No tengo informaci√≥n de proyectos espec√≠fica en este momento. Consulta el m√≥dulo de Proyectos en SADI."
        elif 'meta' in prompt_lower:
            return "üéØ No puedo acceder a las metas actualmente. Revisa el m√≥dulo de Metas en SADI."
        elif 'actividad' in prompt_lower:
            return "‚úÖ No hay informaci√≥n de actividades disponible. Verifica el m√≥dulo de Actividades."
        elif 'ciclo' in prompt_lower:
            return "üîÑ No tengo datos de ciclos. Consulta el m√≥dulo de Programas."
        else:
            return "ü§ñ No tengo informaci√≥n espec√≠fica sobre tu consulta."

# Instancia global
mcp_service = OptimizedSpanishMCPService()
